{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c50dca-5914-43cf-9430-2662440da95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa8233cb-026d-4753-8319-ab893e590863",
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open('doc1.txt', 'r')\n",
    "_text = textfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eae3a3d0-360c-4b4e-aea6-25dd572a93ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural language processing (NLP) is a field of artificial intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language in a way that is both natural and meaningful. NLP techniques are used in various applications, including sentiment analysis, machine translation, text summarization, and chatbots.\\n\\nOne of the key tasks in NLP is text preprocessing, which involves cleaning and transforming raw text data into a format that is suitable for analysis. This often includes tokenization, removing stop words, stemming, and lemmatization.\\n\\nTokenization is the process of splitting text into individual words or tokens. Sentence tokenization, on the other hand, involves splitting text into sentences. Once the text is tokenized, stop wordsâ€”commonly occurring words such as \"the,\" \"is,\" and \"and\"â€”are typically removed to reduce noise in the data.\\n\\nStemming and lemmatization are techniques used to normalize words by reducing them to their root form. Stemming involves removing suffixes from words to produce their stem, while lemmatization maps words to their base or dictionary form.\\n\\nAfter preprocessing the text, various techniques such as term frequency-inverse document frequency (TF-IDF) can be applied to extract meaningful insights from the data. TF-IDF calculates the importance of a word in a document relative to its frequency in the entire corpus, helping to identify keywords and key phrases.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b18905b-d5e8-40fa-9289-2b8d25b9a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "_text = _text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87c09e9f-f7dd-4589-a220-732e1cc484f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural language processing (nlp) is a field of artificial intelligence (ai) that focuses on enabling computers to understand, interpret, and generate human language in a way that is both natural and meaningful. nlp techniques are used in various applications, including sentiment analysis, machine translation, text summarization, and chatbots.\\n\\none of the key tasks in nlp is text preprocessing, which involves cleaning and transforming raw text data into a format that is suitable for analysis. this often includes tokenization, removing stop words, stemming, and lemmatization.\\n\\ntokenization is the process of splitting text into individual words or tokens. sentence tokenization, on the other hand, involves splitting text into sentences. once the text is tokenized, stop wordsâ€”commonly occurring words such as \"the,\" \"is,\" and \"and\"â€”are typically removed to reduce noise in the data.\\n\\nstemming and lemmatization are techniques used to normalize words by reducing them to their root form. stemming involves removing suffixes from words to produce their stem, while lemmatization maps words to their base or dictionary form.\\n\\nafter preprocessing the text, various techniques such as term frequency-inverse document frequency (tf-idf) can be applied to extract meaningful insights from the data. tf-idf calculates the importance of a word in a document relative to its frequency in the entire corpus, helping to identify keywords and key phrases.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d43da08-5f58-4977-942b-66a7a1e2665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8db298b3-37b8-428f-ae0b-73b71b1872eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'nlp',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'field',\n",
       " 'of',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " '(',\n",
       " 'ai',\n",
       " ')',\n",
       " 'that',\n",
       " 'focuses',\n",
       " 'on',\n",
       " 'enabling',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'understand',\n",
       " ',',\n",
       " 'interpret',\n",
       " ',',\n",
       " 'and',\n",
       " 'generate',\n",
       " 'human',\n",
       " 'language',\n",
       " 'in',\n",
       " 'a',\n",
       " 'way',\n",
       " 'that',\n",
       " 'is',\n",
       " 'both',\n",
       " 'natural',\n",
       " 'and',\n",
       " 'meaningful',\n",
       " '.',\n",
       " 'nlp',\n",
       " 'techniques',\n",
       " 'are',\n",
       " 'used',\n",
       " 'in',\n",
       " 'various',\n",
       " 'applications',\n",
       " ',',\n",
       " 'including',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'machine',\n",
       " 'translation',\n",
       " ',',\n",
       " 'text',\n",
       " 'summarization',\n",
       " ',',\n",
       " 'and',\n",
       " 'chatbots',\n",
       " '.',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'key',\n",
       " 'tasks',\n",
       " 'in',\n",
       " 'nlp',\n",
       " 'is',\n",
       " 'text',\n",
       " 'preprocessing',\n",
       " ',',\n",
       " 'which',\n",
       " 'involves',\n",
       " 'cleaning',\n",
       " 'and',\n",
       " 'transforming',\n",
       " 'raw',\n",
       " 'text',\n",
       " 'data',\n",
       " 'into',\n",
       " 'a',\n",
       " 'format',\n",
       " 'that',\n",
       " 'is',\n",
       " 'suitable',\n",
       " 'for',\n",
       " 'analysis',\n",
       " '.',\n",
       " 'this',\n",
       " 'often',\n",
       " 'includes',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'removing',\n",
       " 'stop',\n",
       " 'words',\n",
       " ',',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'and',\n",
       " 'lemmatization',\n",
       " '.',\n",
       " 'tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'splitting',\n",
       " 'text',\n",
       " 'into',\n",
       " 'individual',\n",
       " 'words',\n",
       " 'or',\n",
       " 'tokens',\n",
       " '.',\n",
       " 'sentence',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other',\n",
       " 'hand',\n",
       " ',',\n",
       " 'involves',\n",
       " 'splitting',\n",
       " 'text',\n",
       " 'into',\n",
       " 'sentences',\n",
       " '.',\n",
       " 'once',\n",
       " 'the',\n",
       " 'text',\n",
       " 'is',\n",
       " 'tokenized',\n",
       " ',',\n",
       " 'stop',\n",
       " 'wordsâ€',\n",
       " '”',\n",
       " 'commonly',\n",
       " 'occurring',\n",
       " 'words',\n",
       " 'such',\n",
       " 'as',\n",
       " '``',\n",
       " 'the',\n",
       " ',',\n",
       " \"''\",\n",
       " '``',\n",
       " 'is',\n",
       " ',',\n",
       " \"''\",\n",
       " 'and',\n",
       " '``',\n",
       " 'and',\n",
       " \"''\",\n",
       " 'â€',\n",
       " '”',\n",
       " 'are',\n",
       " 'typically',\n",
       " 'removed',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'noise',\n",
       " 'in',\n",
       " 'the',\n",
       " 'data',\n",
       " '.',\n",
       " 'stemming',\n",
       " 'and',\n",
       " 'lemmatization',\n",
       " 'are',\n",
       " 'techniques',\n",
       " 'used',\n",
       " 'to',\n",
       " 'normalize',\n",
       " 'words',\n",
       " 'by',\n",
       " 'reducing',\n",
       " 'them',\n",
       " 'to',\n",
       " 'their',\n",
       " 'root',\n",
       " 'form',\n",
       " '.',\n",
       " 'stemming',\n",
       " 'involves',\n",
       " 'removing',\n",
       " 'suffixes',\n",
       " 'from',\n",
       " 'words',\n",
       " 'to',\n",
       " 'produce',\n",
       " 'their',\n",
       " 'stem',\n",
       " ',',\n",
       " 'while',\n",
       " 'lemmatization',\n",
       " 'maps',\n",
       " 'words',\n",
       " 'to',\n",
       " 'their',\n",
       " 'base',\n",
       " 'or',\n",
       " 'dictionary',\n",
       " 'form',\n",
       " '.',\n",
       " 'after',\n",
       " 'preprocessing',\n",
       " 'the',\n",
       " 'text',\n",
       " ',',\n",
       " 'various',\n",
       " 'techniques',\n",
       " 'such',\n",
       " 'as',\n",
       " 'term',\n",
       " 'frequency-inverse',\n",
       " 'document',\n",
       " 'frequency',\n",
       " '(',\n",
       " 'tf-idf',\n",
       " ')',\n",
       " 'can',\n",
       " 'be',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'extract',\n",
       " 'meaningful',\n",
       " 'insights',\n",
       " 'from',\n",
       " 'the',\n",
       " 'data',\n",
       " '.',\n",
       " 'tf-idf',\n",
       " 'calculates',\n",
       " 'the',\n",
       " 'importance',\n",
       " 'of',\n",
       " 'a',\n",
       " 'word',\n",
       " 'in',\n",
       " 'a',\n",
       " 'document',\n",
       " 'relative',\n",
       " 'to',\n",
       " 'its',\n",
       " 'frequency',\n",
       " 'in',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'corpus',\n",
       " ',',\n",
       " 'helping',\n",
       " 'to',\n",
       " 'identify',\n",
       " 'keywords',\n",
       " 'and',\n",
       " 'key',\n",
       " 'phrases',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens = nltk.word_tokenize(_text)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d28edc5-8656-4443-820e-379801937183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing (nlp) is a field of artificial intelligence (ai) that focuses on enabling computers to understand, interpret, and generate human language in a way that is both natural and meaningful.',\n",
       " 'nlp techniques are used in various applications, including sentiment analysis, machine translation, text summarization, and chatbots.',\n",
       " 'one of the key tasks in nlp is text preprocessing, which involves cleaning and transforming raw text data into a format that is suitable for analysis.',\n",
       " 'this often includes tokenization, removing stop words, stemming, and lemmatization.',\n",
       " 'tokenization is the process of splitting text into individual words or tokens.',\n",
       " 'sentence tokenization, on the other hand, involves splitting text into sentences.',\n",
       " 'once the text is tokenized, stop wordsâ€”commonly occurring words such as \"the,\" \"is,\" and \"and\"â€”are typically removed to reduce noise in the data.',\n",
       " 'stemming and lemmatization are techniques used to normalize words by reducing them to their root form.',\n",
       " 'stemming involves removing suffixes from words to produce their stem, while lemmatization maps words to their base or dictionary form.',\n",
       " 'after preprocessing the text, various techniques such as term frequency-inverse document frequency (tf-idf) can be applied to extract meaningful insights from the data.',\n",
       " 'tf-idf calculates the importance of a word in a document relative to its frequency in the entire corpus, helping to identify keywords and key phrases.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens = nltk.sent_tokenize(_text)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e695942-37c8-4746-bc09-6c120c9cf268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "stops\n",
    "\n",
    "# st = set(stopwords.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acdc5522-5405-4bea-aac4-689c7f8ba5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  is  a  of  that  on  to  and  in  a  that  is  both  and  are  in  and  of  the  in  is  which  and  into  a  that  is  for  this  and  is  the  of  into  or  on  the  other  into  once  the  is  such  as  the  is  and  and  are  to  in  the  and  are  to  by  them  to  their  from  to  their  while  to  their  or  after  the  such  as  can  be  to  from  the  the  of  a  in  a  to  its  in  the  to  and\n",
      "\n",
      "Filtered text :  natural language processing ( nlp ) field artificial intelligence ( ai ) focuses enabling computers understand , interpret , generate human language way natural meaningful . nlp techniques used various applications , including sentiment analysis , machine translation , text summarization , chatbots . one key tasks nlp text preprocessing , involves cleaning transforming raw text data format suitable analysis . often includes tokenization , removing stop words , stemming , lemmatization . tokenization process splitting text individual words tokens . sentence tokenization , hand , involves splitting text sentences . text tokenized , stop wordsâ€ ” commonly occurring words `` , '' `` , '' `` '' â€ ” typically removed reduce noise data . stemming lemmatization techniques used normalize words reducing root form . stemming involves removing suffixes words produce stem , lemmatization maps words base dictionary form . preprocessing text , various techniques term frequency-inverse document frequency ( tf-idf ) applied extract meaningful insights data . tf-idf calculates importance word document relative frequency entire corpus , helping identify keywords key phrases .\n"
     ]
    }
   ],
   "source": [
    "filtered_words = []\n",
    "\n",
    "for i in word_tokens : \n",
    "    if i in stops : \n",
    "        print(' ',i, end='')\n",
    "    else : \n",
    "        filtered_words.append(i)\n",
    "\n",
    "\n",
    "filtered_text = \" \".join(filtered_words)\n",
    "print('\\n\\nFiltered text : ',filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd1a5d22-1f78-4a67-ac6f-03d4c702ca61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language processing ( nlp ) field artificial intelligence ( ai ) focuses enabling computers understand , interpret , generate human language way natural meaningful . nlp techniques used various applications , including sentiment analysis , machine translation , text summarization , chatbots . one key tasks nlp text preprocessing , involves cleaning transforming raw text data format suitable analysis . often includes tokenization , removing stop words , stemming , lemmatization . tokenization process splitting text individual words tokens . sentence tokenization , hand , involves splitting text sentences . text tokenized , stop wordsâ€ ” commonly occurring words `` , '' `` , '' `` '' â€ ” typically removed reduce noise data . stemming lemmatization techniques used normalize words reducing root form . stemming involves removing suffixes words produce stem , lemmatization maps words base dictionary form . preprocessing text , various techniques term frequency-inverse document frequency ( tf-idf ) applied extract meaningful insights data . tf-idf calculates importance word document relative frequency entire corpus , helping identify keywords key phrases .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "print(porter.stem(filtered_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a88f5b28-04ad-485b-b898-8b138c808021",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language processing ( nlp ) field artificial intelligence ( ai ) focus enabling computer understand , interpret , generate human language way natural meaningful . nlp technique used various application , including sentiment analysis , machine translation , text summarization , chatbots . one key task nlp text preprocessing , involves cleaning transforming raw text data format suitable analysis . often includes tokenization , removing stop word , stemming , lemmatization . tokenization process splitting text individual word token . sentence tokenization , hand , involves splitting text sentence . text tokenized , stop wordsâ€ ” commonly occurring word `` , '' `` , '' `` '' â€ ” typically removed reduce noise data . stemming lemmatization technique used normalize word reducing root form . stemming involves removing suffix word produce stem , lemmatization map word base dictionary form . preprocessing text , various technique term frequency-inverse document frequency ( tf-idf ) applied extract meaningful insight data . tf-idf calculates importance word document relative frequency entire corpus , helping identify keywords key phrase . "
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "lemwords = []\n",
    "for i in filtered_words : \n",
    "    lemword = lem.lemmatize(i)\n",
    "    print(lemword, '', end='')\n",
    "    lemwords.append(lemword)\n",
    "\n",
    "lemtext = \" \".join(lemwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2128f3a-d995-4d0b-9434-96fe09d84ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"natural language processing ( nlp ) field artificial intelligence ( ai ) focus enabling computer understand , interpret , generate human language way natural meaningful . nlp technique used various application , including sentiment analysis , machine translation , text summarization , chatbots . one key task nlp text preprocessing , involves cleaning transforming raw text data format suitable analysis . often includes tokenization , removing stop word , stemming , lemmatization . tokenization process splitting text individual word token . sentence tokenization , hand , involves splitting text sentence . text tokenized , stop wordsâ€ ” commonly occurring word `` , '' `` , '' `` '' â€ ” typically removed reduce noise data . stemming lemmatization technique used normalize word reducing root form . stemming involves removing suffix word produce stem , lemmatization map word base dictionary form . preprocessing text , various technique term frequency-inverse document frequency ( tf-idf ) applied extract meaningful insight data . tf-idf calculates importance word document relative frequency entire corpus , helping identify keywords key phrase .\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c4b637c-8273-4769-bc68-f742093a3a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ai', 'analysis', 'application', 'applied', 'artificial', 'base',\n",
       "       'calculates', 'chatbots', 'cleaning', 'commonly', 'computer',\n",
       "       'corpus', 'data', 'dictionary', 'document', 'enabling', 'entire',\n",
       "       'extract', 'field', 'focus', 'form', 'format', 'frequency',\n",
       "       'generate', 'hand', 'helping', 'human', 'identify', 'idf',\n",
       "       'importance', 'includes', 'including', 'individual', 'insight',\n",
       "       'intelligence', 'interpret', 'inverse', 'involves', 'key',\n",
       "       'keywords', 'language', 'lemmatization', 'machine', 'map',\n",
       "       'meaningful', 'natural', 'nlp', 'noise', 'normalize', 'occurring',\n",
       "       'often', 'one', 'phrase', 'preprocessing', 'process', 'processing',\n",
       "       'produce', 'raw', 'reduce', 'reducing', 'relative', 'removed',\n",
       "       'removing', 'root', 'sentence', 'sentiment', 'splitting', 'stem',\n",
       "       'stemming', 'stop', 'suffix', 'suitable', 'summarization', 'task',\n",
       "       'technique', 'term', 'text', 'tf', 'token', 'tokenization',\n",
       "       'tokenized', 'transforming', 'translation', 'typically',\n",
       "       'understand', 'used', 'various', 'way', 'word', 'wordsâ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer()\n",
    "tmat = tv.fit_transform([lemtext])\n",
    "feature_names = tv.get_feature_names_out()\n",
    "\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3926f51f-f187-490d-99be-5d9023855b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word :  phrase \t\t\t Score :  0.057928444636349226\n",
      "Word :  keywords \t\t\t Score :  0.057928444636349226\n",
      "Word :  identify \t\t\t Score :  0.057928444636349226\n",
      "Word :  helping \t\t\t Score :  0.057928444636349226\n",
      "Word :  corpus \t\t\t Score :  0.057928444636349226\n",
      "Word :  entire \t\t\t Score :  0.057928444636349226\n",
      "Word :  relative \t\t\t Score :  0.057928444636349226\n",
      "Word :  importance \t\t\t Score :  0.057928444636349226\n",
      "Word :  calculates \t\t\t Score :  0.057928444636349226\n",
      "Word :  insight \t\t\t Score :  0.057928444636349226\n",
      "Word :  extract \t\t\t Score :  0.057928444636349226\n",
      "Word :  applied \t\t\t Score :  0.057928444636349226\n",
      "Word :  idf \t\t\t Score :  0.11585688927269845\n",
      "Word :  tf \t\t\t Score :  0.11585688927269845\n",
      "Word :  document \t\t\t Score :  0.11585688927269845\n",
      "Word :  inverse \t\t\t Score :  0.057928444636349226\n",
      "Word :  frequency \t\t\t Score :  0.17378533390904768\n",
      "Word :  term \t\t\t Score :  0.057928444636349226\n",
      "Word :  dictionary \t\t\t Score :  0.057928444636349226\n",
      "Word :  base \t\t\t Score :  0.057928444636349226\n",
      "Word :  map \t\t\t Score :  0.057928444636349226\n",
      "Word :  stem \t\t\t Score :  0.057928444636349226\n",
      "Word :  produce \t\t\t Score :  0.057928444636349226\n",
      "Word :  suffix \t\t\t Score :  0.057928444636349226\n",
      "Word :  form \t\t\t Score :  0.11585688927269845\n",
      "Word :  root \t\t\t Score :  0.057928444636349226\n",
      "Word :  reducing \t\t\t Score :  0.057928444636349226\n",
      "Word :  normalize \t\t\t Score :  0.057928444636349226\n",
      "Word :  noise \t\t\t Score :  0.057928444636349226\n",
      "Word :  reduce \t\t\t Score :  0.057928444636349226\n",
      "Word :  removed \t\t\t Score :  0.057928444636349226\n",
      "Word :  typically \t\t\t Score :  0.057928444636349226\n",
      "Word :  occurring \t\t\t Score :  0.057928444636349226\n",
      "Word :  commonly \t\t\t Score :  0.057928444636349226\n",
      "Word :  wordsâ \t\t\t Score :  0.057928444636349226\n",
      "Word :  tokenized \t\t\t Score :  0.057928444636349226\n",
      "Word :  hand \t\t\t Score :  0.057928444636349226\n",
      "Word :  sentence \t\t\t Score :  0.11585688927269845\n",
      "Word :  token \t\t\t Score :  0.057928444636349226\n",
      "Word :  individual \t\t\t Score :  0.057928444636349226\n",
      "Word :  splitting \t\t\t Score :  0.11585688927269845\n",
      "Word :  process \t\t\t Score :  0.057928444636349226\n",
      "Word :  lemmatization \t\t\t Score :  0.17378533390904768\n",
      "Word :  stemming \t\t\t Score :  0.17378533390904768\n",
      "Word :  word \t\t\t Score :  0.4054991124544446\n",
      "Word :  stop \t\t\t Score :  0.11585688927269845\n",
      "Word :  removing \t\t\t Score :  0.11585688927269845\n",
      "Word :  tokenization \t\t\t Score :  0.17378533390904768\n",
      "Word :  includes \t\t\t Score :  0.057928444636349226\n",
      "Word :  often \t\t\t Score :  0.057928444636349226\n",
      "Word :  suitable \t\t\t Score :  0.057928444636349226\n",
      "Word :  format \t\t\t Score :  0.057928444636349226\n",
      "Word :  data \t\t\t Score :  0.17378533390904768\n",
      "Word :  raw \t\t\t Score :  0.057928444636349226\n",
      "Word :  transforming \t\t\t Score :  0.057928444636349226\n",
      "Word :  cleaning \t\t\t Score :  0.057928444636349226\n",
      "Word :  involves \t\t\t Score :  0.17378533390904768\n",
      "Word :  preprocessing \t\t\t Score :  0.11585688927269845\n",
      "Word :  task \t\t\t Score :  0.057928444636349226\n",
      "Word :  key \t\t\t Score :  0.11585688927269845\n",
      "Word :  one \t\t\t Score :  0.057928444636349226\n",
      "Word :  chatbots \t\t\t Score :  0.057928444636349226\n",
      "Word :  summarization \t\t\t Score :  0.057928444636349226\n",
      "Word :  text \t\t\t Score :  0.4054991124544446\n",
      "Word :  translation \t\t\t Score :  0.057928444636349226\n",
      "Word :  machine \t\t\t Score :  0.057928444636349226\n",
      "Word :  analysis \t\t\t Score :  0.11585688927269845\n",
      "Word :  sentiment \t\t\t Score :  0.057928444636349226\n",
      "Word :  including \t\t\t Score :  0.057928444636349226\n",
      "Word :  application \t\t\t Score :  0.057928444636349226\n",
      "Word :  various \t\t\t Score :  0.11585688927269845\n",
      "Word :  used \t\t\t Score :  0.11585688927269845\n",
      "Word :  technique \t\t\t Score :  0.17378533390904768\n",
      "Word :  meaningful \t\t\t Score :  0.11585688927269845\n",
      "Word :  way \t\t\t Score :  0.057928444636349226\n",
      "Word :  human \t\t\t Score :  0.057928444636349226\n",
      "Word :  generate \t\t\t Score :  0.057928444636349226\n",
      "Word :  interpret \t\t\t Score :  0.057928444636349226\n",
      "Word :  understand \t\t\t Score :  0.057928444636349226\n",
      "Word :  computer \t\t\t Score :  0.057928444636349226\n",
      "Word :  enabling \t\t\t Score :  0.057928444636349226\n",
      "Word :  focus \t\t\t Score :  0.057928444636349226\n",
      "Word :  ai \t\t\t Score :  0.057928444636349226\n",
      "Word :  intelligence \t\t\t Score :  0.057928444636349226\n",
      "Word :  artificial \t\t\t Score :  0.057928444636349226\n",
      "Word :  field \t\t\t Score :  0.057928444636349226\n",
      "Word :  nlp \t\t\t Score :  0.17378533390904768\n",
      "Word :  processing \t\t\t Score :  0.057928444636349226\n",
      "Word :  language \t\t\t Score :  0.11585688927269845\n",
      "Word :  natural \t\t\t Score :  0.11585688927269845\n"
     ]
    }
   ],
   "source": [
    "tdict = {}\n",
    "\n",
    "for col in tmat.nonzero()[1]:\n",
    "    tdict[feature_names[col]] = tmat[0, col]\n",
    "\n",
    "for word, score in tdict.items():\n",
    "    print(\"Word : \",word,\"\\t\\t\\t Score : \",score)\n",
    "\n",
    "# for word, score in sorted(tdict.items(), key=lambda x : x[1], reverse=True):\n",
    "#     print(f\"{word}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f950b24e-b55c-4541-9c81-0cec28333134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ai</th>\n",
       "      <td>0.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analysis</th>\n",
       "      <td>0.115857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>application</th>\n",
       "      <td>0.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>applied</th>\n",
       "      <td>0.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artificial</th>\n",
       "      <td>0.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>0.115857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>various</th>\n",
       "      <td>0.115857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>0.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>0.405499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wordsâ</th>\n",
       "      <td>0.057928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "ai           0.057928\n",
       "analysis     0.115857\n",
       "application  0.057928\n",
       "applied      0.057928\n",
       "artificial   0.057928\n",
       "...               ...\n",
       "used         0.115857\n",
       "various      0.115857\n",
       "way          0.057928\n",
       "word         0.405499\n",
       "wordsâ       0.057928\n",
       "\n",
       "[90 rows x 1 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tfidf = pd.DataFrame(tmat.toarray(), columns=tv.get_feature_names_out())\n",
    "tfidf.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
